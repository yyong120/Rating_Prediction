{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train_set.csv'\n",
    "valid_file = 'valid_set.csv'\n",
    "seen_test_file = 'seen_test_set.csv'\n",
    "unseen_test_file = 'unseen_test_set.csv'\n",
    "test_file = 'test_set.csv'\n",
    "\n",
    "def get_all_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',')\n",
    "    data_list = []\n",
    "\n",
    "    # iterate through the whole dataset\n",
    "    for _, row in df.iterrows():\n",
    "        item = int(row['item_idx'])\n",
    "        user = int(row['user_idx'])\n",
    "        rating = row['rating']\n",
    "\n",
    "        size = int(row['size_idx'])\n",
    "        fit = int(row['fit_idx'])\n",
    "        user_attr = int(row['user_attr_idx'])\n",
    "        model_attr = int(row['model_attr_idx'])\n",
    "        category = int(row['category_idx'])\n",
    "        brand = int(row['brand_idx'])\n",
    "        year = int(row['year_idx'])\n",
    "        split = int(row['split_idx'])\n",
    "\n",
    "        data_list.append([user, item, rating,\n",
    "                          size, fit,\n",
    "                          user_attr, model_attr,\n",
    "                          category, brand,\n",
    "                          year, split])\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "train_data = get_all_data(train_file)\n",
    "\n",
    "valid_data = get_all_data(valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average rating: 4.2093936748808165\n",
      "44683\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "# calculate global average rating on train+valid\n",
    "avg_rating = np.mean([data[2] for data in train_data] + \n",
    "                     [data[2] for data in valid_data]\n",
    "                    )\n",
    "print(f'average rating: {avg_rating}')\n",
    "\n",
    "# get the number of users and items in train+valid\n",
    "n_users = max(max([data[0] for data in train_data]), max([data[0] for data in valid_data]))\n",
    "n_items = max(max([data[1] for data in train_data]), max([data[1] for data in valid_data]))\n",
    "print(n_users)\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset for latent factor model with features\n",
    "class U_I_R_features_Dataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list  # List of [user_idx, item_idx, rating,\n",
    "                                      #        size_idx, fit_idx,\n",
    "                                      #        user_attr_idx, model_attr_idx,\n",
    "                                      #        category_idx, brand_idx,\n",
    "                                      #        year_idx, split_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        u, i, r, size, fit, user_attr, model_attr, category, brand, year, split = self.data_list[idx]\n",
    "        return (torch.tensor(u, dtype=torch.long), torch.tensor(i, dtype=torch.long), torch.tensor(r, dtype=torch.float32),\n",
    "                torch.tensor(size, dtype=torch.long), torch.tensor(fit, dtype=torch.long),\n",
    "                torch.tensor(user_attr, dtype=torch.long), torch.tensor(model_attr, dtype=torch.long),\n",
    "                torch.tensor(category, dtype=torch.long), torch.tensor(brand, dtype=torch.long),\n",
    "                torch.tensor(year, dtype=torch.long), torch.tensor(split, dtype=torch.long),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable parameters\n",
    "K = 5   # dimension of gamma_u / gamma_i\n",
    "alpha_init = avg_rating # maybe no need to tune this\n",
    "\n",
    "lamb_beta_u = 0.01\n",
    "lamb_beta_i = 0.01\n",
    "lamb_gamma_u = 0.01\n",
    "lamb_gamma_i = 0.01\n",
    "\n",
    "# maybe we can define different lambdas for different features\n",
    "lamb_attr_vec = 0.01\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 1024\n",
    "max_train_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and dataloader\n",
    "train_dataset = U_I_R_features_Dataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# valid data should be in one batch\n",
    "valid_dataset = U_I_R_features_Dataset(valid_data)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_data), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModelWithFeatures(nn.Module):\n",
    "    def __init__(self, alpha_init, K, lamb_beta_u, lamb_beta_i, lamb_gamma_u, lamb_gamma_i, lamb_attr_vec,\n",
    "                 num_users, num_items,\n",
    "                 n_size=9, n_fit=5,\n",
    "                 n_user_attr=2, n_model_attr=2,\n",
    "                 n_category=4, n_brand=31,\n",
    "                 n_year=10, n_split=3):\n",
    "        super(LatentFactorModelWithFeatures, self).__init__()\n",
    "\n",
    "        # Initialize scalar average rating\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha_init, dtype=torch.float32))\n",
    "\n",
    "        # Bias terms for users and items\n",
    "        # user/item idx = 0 means unseen user/item\n",
    "        self.betaU = nn.Embedding(num_users+1, 1, padding_idx=0)\n",
    "        self.betaI = nn.Embedding(num_items+1, 1, padding_idx=0)\n",
    "\n",
    "        # Latent factors for users and items\n",
    "        self.gammaU = nn.Embedding(num_users+1, K, padding_idx=0)\n",
    "        self.gammaI = nn.Embedding(num_items+1, K, padding_idx=0)\n",
    "\n",
    "        # attribute parameters for features\n",
    "        self.rhoSize = nn.Embedding(n_size+1, K, padding_idx=0)\n",
    "        self.rhoFit = nn.Embedding(n_fit+1, K, padding_idx=0)\n",
    "        self.rhoUserAttr = nn.Embedding(n_user_attr+1, K, padding_idx=0)\n",
    "        self.rhoModelAttr = nn.Embedding(n_model_attr+1, K, padding_idx=0)\n",
    "        self.rhoCategory = nn.Embedding(n_category+1, K, padding_idx=0)\n",
    "        self.rhoBrand = nn.Embedding(n_brand+1, K, padding_idx=0)\n",
    "        self.rhoYear = nn.Embedding(n_year, K, padding_idx=0)    # year has no nan value\n",
    "        self.rhoSplit = nn.Embedding(n_split, K, padding_idx=0)  # split has no nan value\n",
    "        \n",
    "        self.lamb_beta_u = lamb_beta_u\n",
    "        self.lamb_beta_i = lamb_beta_i\n",
    "        self.lamb_gamma_u = lamb_gamma_u\n",
    "        self.lamb_gamma_i = lamb_gamma_i\n",
    "        self.lamb_attr_vec = lamb_attr_vec\n",
    "\n",
    "        # Initialize embeddings with small random values\n",
    "        # If training doesn't converge, try other initialization strategies, e.g. initialize all parameters to 0\n",
    "        nn.init.normal_(self.betaU.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.betaI.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.gammaU.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.gammaI.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoSize.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoFit.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoUserAttr.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoModelAttr.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoCategory.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoBrand.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoYear.weight, mean=0.0, std=0.001)\n",
    "        nn.init.normal_(self.rhoSplit.weight, mean=0.0, std=0.001)\n",
    "\n",
    "    # u / i, features should have the shape of (b,)\n",
    "    def predict(self, u, i, size, fit, user_attr, model_attr, category, brand, year, split):\n",
    "        beta_u = self.betaU(u).squeeze()    # (b,)\n",
    "        beta_i = self.betaI(i).squeeze()    # (b,)\n",
    "        gamma_u = self.gammaU(u)            # (b, K)\n",
    "        gamma_i = self.gammaI(i)            # (b, K)\n",
    "\n",
    "        # all feature parameters have the shape of (b, k)\n",
    "        rho_size = self.rhoSize(size)\n",
    "        rho_fit = self.rhoFit(fit)\n",
    "        rho_user_attr = self.rhoUserAttr(user_attr)\n",
    "        rho_model_attr = self.rhoModelAttr(model_attr)\n",
    "        rho_category = self.rhoCategory(category)\n",
    "        rho_brand = self.rhoBrand(brand)\n",
    "        rho_year = self.rhoYear(year)\n",
    "        rho_split = self.rhoSplit(split)\n",
    "\n",
    "        p = self.alpha + beta_u + beta_i + torch.sum((gamma_u +\n",
    "                                                      rho_size + rho_fit +\n",
    "                                                      rho_user_attr + rho_model_attr +\n",
    "                                                      rho_category + rho_brand +\n",
    "                                                      rho_year + rho_split) * \n",
    "                                                    gamma_i, dim=-1)\n",
    "        return p    # (b,)\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return (\n",
    "            self.lamb_beta_u * torch.sum(self.betaU.weight**2) +\n",
    "            self.lamb_beta_i * torch.sum(self.betaI.weight**2) +\n",
    "            self.lamb_gamma_u * torch.sum(self.gammaU.weight**2) +\n",
    "            self.lamb_gamma_i * torch.sum(self.gammaI.weight**2) + \n",
    "            self.lamb_attr_vec * (torch.sum(self.rhoSize.weight**2) +\n",
    "                                  torch.sum(self.rhoFit.weight**2) +\n",
    "                                  torch.sum(self.rhoUserAttr.weight**2) +\n",
    "                                  torch.sum(self.rhoModelAttr.weight**2) +\n",
    "                                  torch.sum(self.rhoCategory.weight**2) +\n",
    "                                  torch.sum(self.rhoBrand.weight**2) +\n",
    "                                  torch.sum(self.rhoYear.weight**2) +\n",
    "                                  torch.sum(self.rhoSplit.weight**2)\n",
    "                                  )\n",
    "        )\n",
    "\n",
    "    # Loss\n",
    "    # u, i, r, features should have the shape of (b,)\n",
    "    def forward(self, u, i, r, size, fit, user_attr, model_attr, category, brand, year, split):\n",
    "        pred = self.predict(u, i, size, fit, user_attr, model_attr, category, brand, year, split)\n",
    "        # r = torch.tensor(r, dtype=torch.float32)\n",
    "        return torch.nn.functional.mse_loss(pred, r, reduction='mean')\n",
    "\n",
    "\n",
    "# evaluate on valid/test set\n",
    "# valid/test set dataloader should only have one batch\n",
    "# u, i should be torch.long tensors with the shape of (b,)\n",
    "# r should be torch.float32 tensors with the shape of (b,)\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    u, i, r, size, fit, user_attr, model_attr, category, brand, year, split = next(iter(dataloader))\n",
    "    pred = model.predict(u, i, size, fit, user_attr, model_attr, category, brand, year, split)  # (b,)\n",
    "\n",
    "    model.train()\n",
    "    return torch.nn.functional.mse_loss(pred, r, reduction='mean').item()\n",
    "\n",
    "\n",
    "# training function\n",
    "# early stop if mse of valid set starts to increase\n",
    "def training_step(model, dataloader, optimizer, valid_dataloader, pre_valid_mse=None):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    early_stop = False\n",
    "\n",
    "    for u, i, r, size, fit, user_attr, model_attr, category, brand, year, split in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass: calculate the predicted ratings\n",
    "        loss = model(u, i, r, size, fit, user_attr, model_attr, category, brand, year, split)  # Model forward pass\n",
    "        loss += model.reg()  # Add regularization loss\n",
    "        \n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Optimizer step: update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # evaluate on valid set to check if we need to early stop\n",
    "        valid_mse = evaluate(model, valid_dataloader)\n",
    "        print(f\"valid_mse: {valid_mse:.4f}\")\n",
    "        if (pre_valid_mse is not None and pre_valid_mse < valid_mse) or np.isnan(valid_mse):\n",
    "            early_stop = True\n",
    "            break\n",
    "        pre_valid_mse = valid_mse\n",
    "\n",
    "    # Return average loss per batch, early stop, valid_mse\n",
    "    return total_loss / len(dataloader), early_stop, valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_mse: 1.1822\n",
      "valid_mse: 1.1805\n",
      "valid_mse: 1.1786\n",
      "valid_mse: 1.1764\n",
      "valid_mse: 1.1733\n",
      "valid_mse: 1.1701\n",
      "valid_mse: 1.1669\n",
      "valid_mse: 1.1641\n",
      "valid_mse: 1.1610\n",
      "valid_mse: 1.1580\n",
      "valid_mse: 1.1550\n",
      "valid_mse: 1.1526\n",
      "valid_mse: 1.1498\n",
      "valid_mse: 1.1476\n",
      "valid_mse: 1.1458\n",
      "valid_mse: 1.1443\n",
      "valid_mse: 1.1430\n",
      "valid_mse: 1.1417\n",
      "valid_mse: 1.1406\n",
      "valid_mse: 1.1398\n",
      "valid_mse: 1.1395\n",
      "valid_mse: 1.1393\n",
      "valid_mse: 1.1395\n",
      "Early stop at epoch 1, valid_mse = 1.1395\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "modelLFMWithFeatures = LatentFactorModelWithFeatures(alpha_init, K, lamb_beta_u, lamb_beta_i, lamb_gamma_u, lamb_gamma_i, lamb_attr_vec, n_users, n_items)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(modelLFMWithFeatures.parameters(), lr=learning_rate)\n",
    "\n",
    "# train loop\n",
    "pre_valid_mse = None\n",
    "for epoch in range(max_train_step):\n",
    "    avg_loss, early_stop, valid_mse = training_step(modelLFMWithFeatures, train_dataloader, optimizer, valid_dataloader, pre_valid_mse)\n",
    "    if early_stop:\n",
    "        print(f\"Early stop at epoch {epoch + 1}, valid_mse = {valid_mse:.4f}\")\n",
    "        break\n",
    "    if epoch % 10 == 9:\n",
    "        print(f\"Epoch {epoch + 1}, average loss = {avg_loss:.4f}\")\n",
    "    pre_valid_mse = valid_mse\n",
    "\n",
    "# save model\n",
    "model_path = 'model/LFM_with_features.pth'\n",
    "torch.save(modelLFMWithFeatures, model_path)\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "evalute on test set (seen test + unseen test)\n",
      "test_mse: 1.1153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load model\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    # evaluate on test set\n",
    "    print(\"evalute on test set (seen test + unseen test)\")\n",
    "    test_data = get_all_data(test_file)\n",
    "\n",
    "    test_dataset = U_I_R_features_Dataset(test_data)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_data), shuffle=True)\n",
    "\n",
    "    test_mse = evaluate(model, test_dataloader)\n",
    "    print(f\"test_mse: {test_mse:.4f}\")\n",
    "else:\n",
    "    print(\"model doesn't exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
