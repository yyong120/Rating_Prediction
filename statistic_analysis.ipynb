{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'df_modcloth.csv'\n",
    "df = pd.read_csv(file_name, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>size</th>\n",
       "      <th>fit</th>\n",
       "      <th>user_attr</th>\n",
       "      <th>model_attr</th>\n",
       "      <th>category</th>\n",
       "      <th>brand</th>\n",
       "      <th>year</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7443</td>\n",
       "      <td>Alex</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-21 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small</td>\n",
       "      <td>Small</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7443</td>\n",
       "      <td>carolyn.agan</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-27 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7443</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-29 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small</td>\n",
       "      <td>Small</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7443</td>\n",
       "      <td>De</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-02-13 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7443</td>\n",
       "      <td>tasha</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-02-18 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small</td>\n",
       "      <td>Small</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id       user_id  rating                  timestamp  size  fit  \\\n",
       "0     7443          Alex       4  2010-01-21 08:00:00+00:00   NaN  NaN   \n",
       "1     7443  carolyn.agan       3  2010-01-27 08:00:00+00:00   NaN  NaN   \n",
       "2     7443         Robyn       4  2010-01-29 08:00:00+00:00   NaN  NaN   \n",
       "3     7443            De       4  2010-02-13 08:00:00+00:00   NaN  NaN   \n",
       "4     7443         tasha       4  2010-02-18 08:00:00+00:00   NaN  NaN   \n",
       "\n",
       "  user_attr model_attr category brand  year  split  \n",
       "0     Small      Small  Dresses   NaN  2012      0  \n",
       "1       NaN      Small  Dresses   NaN  2012      0  \n",
       "2     Small      Small  Dresses   NaN  2012      0  \n",
       "3       NaN      Small  Dresses   NaN  2012      0  \n",
       "4     Small      Small  Dresses   NaN  2012      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99893, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item_id': 7443, 'user_id': 'Alex', 'rating': 4, 'timestamp': '2010-01-21 08:00:00+00:00', 'size': nan, 'fit': nan, 'user_attr': 'Small', 'model_attr': 'Small', 'category': 'Dresses', 'brand': nan, 'year': 2012, 'split': 0}\n"
     ]
    }
   ],
   "source": [
    "# print the first row\n",
    "row = df.iloc[0]\n",
    "print(f\"{row.to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check possible values for different fields\n",
    "items = df['item_id'].unique().tolist()\n",
    "users = df['user_id'].unique().tolist()\n",
    "ratings = df['rating'].unique().tolist()\n",
    "sizes = df['size'].unique().tolist()\n",
    "fits = df['fit'].unique().tolist()\n",
    "user_attrs = df['user_attr'].unique().tolist()\n",
    "model_attrs = df['model_attr'].unique().tolist()\n",
    "categories = df['category'].unique().tolist()\n",
    "brands = df['brand'].unique().tolist()\n",
    "years = df['year'].unique().tolist()\n",
    "splits = df['split'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 1020\n",
      "number of user: 44784\n",
      "number of brands: 32\n",
      "brands: [nan, 'ModCloth', 'Retrolicious', 'Steve Madden', 'Ryu', 'Chi Chi London', 'Out of Print', 'Kin Ship', 'Jack by BB Dakota', 'Pink Martini', 'Miss Candyfloss', 'Emily and Fin', 'Daisey Natives', 'Hell Bunny', 'Banned', 'Sugarhill Boutique', 'Wrangler', 'Wendy Bird', 'Pepaloves', 'Collectif', 'Compania Fantastica', 'Closet London', 'Eliza J', 'BB Dakota', \"Alice's Pig\", 'Louche', \"Effie's Heart\", 'Miss Patina', 'Mata Traders', \"Rolla's\", 'Yumi', 'Blue Platypus']\n",
      "ratings: [4, 3, 5, 2, 1]\n",
      "sizes: [nan, 1.0, 2.0, 3.0, 7.0, 4.0, 6.0, 5.0, 8.0, 0.0]\n",
      "fits: [nan, 'Just right', 'Slightly small', 'Very small', 'Slightly large', 'Very large']\n",
      "user_attrs: ['Small', nan, 'Large']\n",
      "model_attrs: ['Small', 'Small&Large']\n",
      "categories: ['Dresses', 'Outerwear', 'Bottoms', 'Tops']\n",
      "years: [2012, 2010, 2011, 2013, 2014, 2016, 2015, 2018, 2017, 2019]\n",
      "splits: [0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of items: {len(items)}\")\n",
    "print(f\"number of user: {len(users)}\")\n",
    "print(f\"number of brands: {len(brands)}\")\n",
    "print(f\"brands: {brands}\")\n",
    "print(f\"ratings: {ratings}\")\n",
    "print(f\"sizes: {sizes}\")\n",
    "print(f\"fits: {fits}\")\n",
    "print(f\"user_attrs: {user_attrs}\")\n",
    "print(f\"model_attrs: {model_attrs}\")\n",
    "print(f\"categories: {categories}\")\n",
    "print(f\"years: {years}\")\n",
    "print(f\"splits: {splits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of data: 99893\n",
      "number of data which doesn't have brand info: 73980\n"
     ]
    }
   ],
   "source": [
    "# check how many data don't have brand info\n",
    "brand_list = df['brand'].tolist()\n",
    "print(f\"total number of data: {len(brand_list)}\")\n",
    "print(f\"number of data which doesn't have brand info: {len([brand for brand in brand_list if pd.isna(brand)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of data: 99893\n",
      "number of nan user_id: 1\n",
      "total number of data: 99893\n",
      "number of nan item_id: 0\n"
     ]
    }
   ],
   "source": [
    "# check how many user_id is nan\n",
    "user_list = df['user_id'].to_list()\n",
    "print(f\"total number of data: {len(user_list)}\")\n",
    "print(f\"number of nan user_id: {len([user for user in user_list if pd.isna(user)])}\")\n",
    "\n",
    "# check how many item_id is nan\n",
    "item_list = df['item_id'].to_list()\n",
    "print(f\"total number of data: {len(item_list)}\")\n",
    "print(f\"number of nan item_id: {len([item for item in item_list if pd.isna(item)])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_to_idx = {'Just right': 1, 'Slightly small': 2, 'Very small': 3, 'Slightly large': 4, 'Very large': 5}\n",
    "user_attr_to_idx = {'Small': 1, 'Large': 2}\n",
    "model_attr_to_idx = {'Small': 1, 'Small&Large': 2}\n",
    "category_to_idx = {'Dresses': 1, 'Outerwear': 2, 'Bottoms': 3, 'Tops': 4}\n",
    "\n",
    "# map brand name to idx, idx for nan is 0\n",
    "brand_to_idx = {}\n",
    "idx = 1\n",
    "for brand in brands:\n",
    "    if not pd.isna(brand):\n",
    "        brand_to_idx[brand] = idx\n",
    "        idx += 1\n",
    "\n",
    "# map user_id to idx, starting from 1\n",
    "user_id_to_idx = {}\n",
    "idx = 1\n",
    "for user in users:\n",
    "    if not pd.isna(user):\n",
    "        user_id_to_idx[user] = idx\n",
    "        idx += 1\n",
    "\n",
    "# map item_id to idx, starting from 1\n",
    "item_id_to_idx = {}\n",
    "idx = 1\n",
    "for item in items:\n",
    "    if not pd.isna(item):\n",
    "        item_id_to_idx[item] = idx\n",
    "        idx += 1\n",
    "\n",
    "# group the data by user\n",
    "# each data is a tuple (item_idx, rating, size_idx, fit_idx, user_attr_idx, model_attr_idx, category_idx, brand_idx, year_idx, split_idx)\n",
    "data_per_user = defaultdict(list)\n",
    "\n",
    "# group the data by item\n",
    "# each data is a tuple (user_idx, rating, size_idx, fit_idx, user_attr_idx, model_attr_idx, category_idx, brand_idx, year_idx, split_idx)\n",
    "# data_per_item = defaultdict(list)\n",
    "\n",
    "# iterate through the whole dataset\n",
    "for _, row in df.iterrows():\n",
    "    item = row['item_id']\n",
    "    user = row['user_id']\n",
    "    if pd.isna(item) or pd.isna(user):\n",
    "        continue\n",
    "\n",
    "    # convert features to int idx\n",
    "    item_idx = item_id_to_idx[item]\n",
    "    user_idx = user_id_to_idx[user]\n",
    "    rating = row['rating']\n",
    "    \n",
    "    size = row['size']\n",
    "    size_idx = 0\n",
    "    if not pd.isna(size):\n",
    "        size_idx = int(size) + 1    # mapping to 1-9\n",
    "    \n",
    "    fit = row['fit']\n",
    "    fit_idx = 0\n",
    "    if not pd.isna(fit):\n",
    "        fit_idx = fit_to_idx[fit]\n",
    "\n",
    "    user_attr = row['user_attr']\n",
    "    user_attr_idx = 0\n",
    "    if not pd.isna(user_attr):\n",
    "        user_attr_idx = user_attr_to_idx[user_attr]\n",
    "    \n",
    "    model_attr = row['model_attr']\n",
    "    model_attr_idx = 0\n",
    "    if not pd.isna(model_attr):\n",
    "        model_attr_idx = model_attr_to_idx[model_attr]\n",
    "    \n",
    "    category = row['category']\n",
    "    category_idx = 0\n",
    "    if not pd.isna(category):\n",
    "        category_idx = category_to_idx[category]\n",
    "    \n",
    "    brand = row['brand']\n",
    "    brand_idx = 0\n",
    "    if not pd.isna(brand):\n",
    "        brand_idx = brand_to_idx[brand]\n",
    "    \n",
    "    year_idx = row['year'] - 2010\n",
    "\n",
    "    split_idx = row['split']\n",
    "\n",
    "    # store the data tuple in the corresponding user/item group\n",
    "    data_per_user[user_idx].append((item_idx, rating, size_idx, fit_idx, user_attr_idx, model_attr_idx, category_idx, brand_idx, year_idx, split_idx))\n",
    "    # data_per_item[item_idx].append((user_idx, rating, size_idx, fit_idx, user_attr_idx, model_attr_idx, category_idx, brand_idx, year_idx, split_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users in unseen test set: 100\n"
     ]
    }
   ],
   "source": [
    "# sort users by the number of items they purchased\n",
    "user_buy_counts = []\n",
    "for user, datas in data_per_user.items():\n",
    "    user_buy_counts.append((len(datas), user))\n",
    "\n",
    "# sort users by the number of items they purchased\n",
    "user_buy_counts.sort(reverse=True)\n",
    "\n",
    "# put the last 100 users' data in unseen test set to ensure there are unseen users in test set.\n",
    "unseen_test_users = set([user for _, user in user_buy_counts[-100:]])\n",
    "print(f'number of users in unseen test set: {len(unseen_test_users)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each remaining user, 20% data in test set, 10% data in validation set, 70% data in training set\n",
    "# calculate global average on training set + validation set\n",
    "unseen_test_set_per_user = {}\n",
    "seen_test_set_per_user = {}\n",
    "train_set_per_user = {}\n",
    "valid_set_per_user = {}\n",
    "\n",
    "for user, datas in data_per_user.items():\n",
    "    if user in unseen_test_users:\n",
    "        unseen_test_set_per_user[user] = datas[:]\n",
    "        continue\n",
    "    random.shuffle(datas)\n",
    "    n_data = len(datas)\n",
    "    test_set = datas[:int(n_data * 0.25):]\n",
    "    train_valid_set = datas[int(n_data * 0.25):]\n",
    "\n",
    "    valid_set = train_valid_set[:int(n_data * 0.15)]\n",
    "    train_set = train_valid_set[int(n_data * 0.15):]\n",
    "\n",
    "    if len(train_set) == 0:\n",
    "        train_set = datas[:]\n",
    "    \n",
    "    train_set_per_user[user] = train_set\n",
    "\n",
    "    if len(valid_set) > 0:\n",
    "        valid_set_per_user[user] = valid_set\n",
    "    \n",
    "    if len(test_set) > 0:\n",
    "        seen_test_set_per_user[user] = test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global average rating on train+valid\n",
    "# calculate user average rating per user on train+valid\n",
    "# calculate item average rating per item on train+valid\n",
    "\n",
    "ratings_train_valid = []\n",
    "users_train_valid = set()\n",
    "items_train_valid = set()\n",
    "\n",
    "user_ratings_train_valid = defaultdict(list)\n",
    "item_ratings_train_valid = defaultdict(list)\n",
    "\n",
    "# iterate through training set\n",
    "for user, datas in train_set_per_user.items():\n",
    "    users_train_valid.add(user)\n",
    "    for d in datas:\n",
    "        item = d[0]\n",
    "        rating = d[1]\n",
    "        items_train_valid.add(item)\n",
    "        ratings_train_valid.append(rating)\n",
    "        user_ratings_train_valid[user].append(rating)\n",
    "        item_ratings_train_valid[item].append(rating)\n",
    "\n",
    "# iterate through validation set\n",
    "for user, datas in valid_set_per_user.items():\n",
    "    users_train_valid.add(user)\n",
    "    for d in datas:\n",
    "        item = d[0]\n",
    "        rating = d[1]\n",
    "        items_train_valid.add(item)\n",
    "        ratings_train_valid.append(rating)\n",
    "        user_ratings_train_valid[user].append(rating)\n",
    "        item_ratings_train_valid[item].append(rating)\n",
    "\n",
    "avg_rating_train_valid = np.mean(ratings_train_valid)\n",
    "\n",
    "user_avg_rating_train_valid = {}\n",
    "item_avg_rating_train_valid = {}\n",
    "\n",
    "for user, ratings in user_ratings_train_valid.items():\n",
    "    user_avg_rating_train_valid[user] = np.mean(ratings)\n",
    "\n",
    "for item, ratings in item_ratings_train_valid.items():\n",
    "    item_avg_rating_train_valid[item] = np.mean(ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44683\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "print(len(users_train_valid))\n",
    "print(len(items_train_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_test_set = []\n",
    "seen_test_set = []\n",
    "train_set = []\n",
    "valid_set = []\n",
    "\n",
    "# each data tuple should be (item_idx, user_idx, rating, size_idx, fit_idx, user_attr_idx, model_attr_idx, category_idx, brand_idx, year_idx, split_idx, user_avg_rating, item_avg_rating)\n",
    "\n",
    "def add_data_to_list(dataset_per_user, dataset_list):\n",
    "    for user, datas in dataset_per_user.items():\n",
    "        for d in datas:\n",
    "            item = d[0]\n",
    "            \n",
    "            user_avg_rating = avg_rating_train_valid\n",
    "            if user in users_train_valid:\n",
    "                user_avg_rating = user_avg_rating_train_valid[user]\n",
    "\n",
    "            item_avg_rating = avg_rating_train_valid\n",
    "            if item in items_train_valid:\n",
    "                item_avg_rating = item_avg_rating_train_valid[item]\n",
    "            \n",
    "            dataset_list.append((item, user) + d[1:] + (user_avg_rating, item_avg_rating))\n",
    "\n",
    "add_data_to_list(unseen_test_set_per_user, unseen_test_set)\n",
    "add_data_to_list(seen_test_set_per_user, seen_test_set)\n",
    "add_data_to_list(train_set_per_user, train_set)\n",
    "add_data_to_list(valid_set_per_user, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-index the user_idx and item_idx in train+valid set, starting from 1\n",
    "# if a user/item is not seen in the train+valid set, its idx should be 0\n",
    "\n",
    "user_idx_old_to_new = {}\n",
    "item_idx_old_to_new = {}\n",
    "\n",
    "for new_idx, old_idx in enumerate(list(users_train_valid), start=1):\n",
    "    user_idx_old_to_new[old_idx] = new_idx\n",
    "\n",
    "for new_idx, old_idx in enumerate(list(items_train_valid), start=1):\n",
    "    item_idx_old_to_new[old_idx] = new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each data tuple should be (item_idx, user_idx, rating, size_idx, fit_idx, user_attr_idx, model_attr_idx, category_idx, brand_idx, year_idx, split_idx, user_avg_rating, item_avg_rating)\n",
    "def reindex(dataset_list):\n",
    "    for i in range(len(dataset_list)):\n",
    "        data = dataset_list[i]\n",
    "        \n",
    "        old_item_idx = data[0]\n",
    "        old_user_idx = data[1]\n",
    "\n",
    "        new_item_idx = 0\n",
    "        new_user_idx = 0\n",
    "\n",
    "        if old_item_idx in items_train_valid:\n",
    "            new_item_idx = item_idx_old_to_new[old_item_idx]\n",
    "        \n",
    "        if old_user_idx in users_train_valid:\n",
    "            new_user_idx = user_idx_old_to_new[old_user_idx]\n",
    "        \n",
    "        dataset_list[i] = (new_item_idx, new_user_idx) + data[2:]\n",
    "\n",
    "reindex(train_set)\n",
    "reindex(valid_set)\n",
    "reindex(unseen_test_set)\n",
    "reindex(seen_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84811\n",
      "4547\n",
      "100\n",
      "10434\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(train_set)\n",
    "random.shuffle(valid_set)\n",
    "random.shuffle(unseen_test_set)\n",
    "random.shuffle(seen_test_set)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(valid_set))\n",
    "print(len(unseen_test_set))\n",
    "print(len(seen_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in train set\n",
      "num of unseen users: 0, num of unseen items: 0\n",
      "in valid set\n",
      "num of unseen users: 0, num of unseen items: 0\n",
      "in unseen test set\n",
      "num of unseen users: 100, num of unseen items: 0\n",
      "in seen test set\n",
      "num of unseen users: 0, num of unseen items: 2\n"
     ]
    }
   ],
   "source": [
    "# check if there are unseen users/items in different datasets\n",
    "def check_num_unseen(dataset):\n",
    "    n_unseen_users = len([data for data in dataset if data[1] == 0])\n",
    "    n_unseen_items = len([data for data in dataset if data[0] == 0])\n",
    "    print(f'num of unseen users: {n_unseen_users}, num of unseen items: {n_unseen_items}')\n",
    "\n",
    "print(\"in train set\")\n",
    "check_num_unseen(train_set)\n",
    "\n",
    "print(\"in valid set\")\n",
    "check_num_unseen(valid_set)\n",
    "\n",
    "print(\"in unseen test set\")\n",
    "check_num_unseen(unseen_test_set)\n",
    "\n",
    "print(\"in seen test set\")\n",
    "check_num_unseen(seen_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write preprocessd train set file, validation set file, test set file\n",
    "# test set = seen (user) test set + unseen (user) test set\n",
    "\n",
    "columns = [\n",
    "    \"item_idx\", \"user_idx\", \"rating\", \"size_idx\", \"fit_idx\", \n",
    "    \"user_attr_idx\", \"model_attr_idx\", \"category_idx\", \n",
    "    \"brand_idx\", \"year_idx\", \"split_idx\", \"user_avg_rating\", \"item_avg_rating\"\n",
    "]\n",
    "\n",
    "\n",
    "def write_data_to_csv(csv_file_name, datasets):\n",
    "    with open(csv_file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        writer.writerow(columns)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            writer.writerows(dataset)\n",
    "            \n",
    "train_file = 'train_set.csv'\n",
    "valid_file = 'valid_set.csv'\n",
    "seen_test_file = 'seen_test_set.csv'\n",
    "unseen_test_file = 'unseen_test_set.csv'\n",
    "test_file = 'test_set.csv'\n",
    "\n",
    "write_data_to_csv(train_file, [train_set])\n",
    "write_data_to_csv(valid_file, [valid_set])\n",
    "write_data_to_csv(seen_test_file, [seen_test_set])\n",
    "write_data_to_csv(unseen_test_file, [unseen_test_set])\n",
    "write_data_to_csv(test_file, [seen_test_set, unseen_test_set])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
